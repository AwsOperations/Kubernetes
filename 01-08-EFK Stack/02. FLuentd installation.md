
# Fluentd installation in kubernetes EKS.

1. We need three files to make fluentd run inside EKS worker nodes
2. Please take a look at the files inside fluentd.yaml where we need to give your elasticsearch credentials
3. inside fluentd daemonset yaml file and as well as fluend-configmap.yaml replace your elasticsearch username and password accordingly.
4. Install kubectl and update your eks config file to kubectl. using below command

$ aws eks update-kubeconfig --region us-east-1 --name eksdemo1

# Please clone the files into your repo

1. Fluentd.yaml 		------> daemonset file to run on each worker node
2. FLuentd-configmap		------> configmap file to get attached to daemonset
3. Fluentd-rbac			------> Rbac roles file to give permissions to fluentd to access the pods

## Step 01: First We need to create namespace for fluentd

$ kubectl create ns fluentd

## Step 02: we need to know what's inside fluentd-configmap.yaml file

We have 3 sections in our fluentd-configmap.yaml :

1. From where we need to collect the logs source is /var/log/containers/*.log
2. Apply filters (filter kubernetes.**)
3. Elasticsearch user credentials (@type elasticsearch) you can change your elasticsearch ip and credentials accordingly

## Step 03: Now get inside the folders of Fluentd and start applying it.

$ cd fluentd/

$ kubectl apply -f fluentd-configmap.yaml

$ kubectl apply -f fluentd-rbac.yaml 

$ kubectl apply -f fluentd.yaml

$ kubectl -n fluentd get pods  			--------> checking the pods which is created under fluentd namespace

## Step 04: Let's deploy our example app that writes logs to stdout so that fluentd will collect logs and ship it to elasticsearch.

$ cd fluentd/

$ kubectl apply -f counter.yaml

## Step 05: Now you need to allow some security group rules from eks and ec2 machines

To configure the security groups for communication between Fluentd running inside an Amazon EKS cluster and Elasticsearch/Kibana running on an EC2 instance, you can follow these steps:

Identify the security groups:

Obtain the security group ID of the EC2 instance where Elasticsearch and Kibana are running.
Identify the security group associated with the EKS cluster or the worker nodes.

1. Inbound rules for the EC2 instance security group:

Allow incoming traffic on the Elasticsearch and Kibana ports (default ports are 9200 for Elasticsearch and 5601 for Kibana) from the security group associated with the EKS cluster or worker nodes.
Add an inbound rule to allow SSH access (port 22) if required for management purposes.
Outbound rules for the EC2 instance security group:

2. Allow outgoing traffic to the IP range of the EKS cluster or worker nodes.
You can specify the security group associated with the EKS cluster as the source.
Inbound rules for the EKS cluster/worker nodes security group:

Allow incoming traffic on the Fluentd port (e.g., port 24224) from the security group associated with the EC2 instance running Elasticsearch/Kibana.
Outbound rules for the EKS cluster/worker nodes security group:

Allow outgoing traffic to the IP and port range of the EC2 instance where Elasticsearch/Kibana is running. You can specify the security group associated with the EC2 instance as the destination.
Ensure that the security groups are properly applied to the respective resources. These configurations will allow communication between Fluentd and Elasticsearch/Kibana, enabling Fluentd to forward logs from the EKS cluster to Elasticsearch/Kibana running on the EC2 instance.

Note: The specific steps may vary depending on your AWS setup and security group configurations. Adjust the rules accordingly based on your network and security requirements.

## Step 06: Check whether the logs are shipped to elasticsearch by logging into kibana stack management and check the index

Fluentd installation in kubernetes EKS.

## Step 07: Issues 

# IF your eks is not updated with the kubectl file please use the below commands and configure your authentication

$ aws eks update-kubeconfig --region <region name> --name <cluster name>

$ aws eks update-kubeconfig --region us-east-1 --name eksdemo1

-- send terraform-key.pem (your .pem key file) to elasticsearch machine and install kubectl and update the above command 
and start deployment

-- still facing problem check with security groups.

-- check with your aws configure credentials

$ aws sts get-caller-identity

-- check fluentd logs by using below commands

$ kubectl logs -n fluentd <fluentd-pod-name>

$ kubectl logs -n fluentd fluentd-xbh8t













